{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def get_bbox(obj):\n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def get_label(obj):\n",
    "    if obj.find('name').text == \"with_mask\":ssh\n",
    "        return 2\n",
    "    elif obj.find('name').text == \"mask_weared_incorrect\":\n",
    "        return 3\n",
    "    return 1\n",
    "\n",
    "def get_sample(path, image_id): \n",
    "    with open(f'{path}/annotations/maksssksksss{image_id}.xml') as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "\n",
    "        num_objs = len(objects)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(get_bbox(i))\n",
    "            labels.append(get_label(i))\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        sample = {}\n",
    "        sample[\"boxes\"] = boxes\n",
    "        sample[\"labels\"] = labels\n",
    "        sample['image'] = np.array(Image.open(f'images/maksssksksss{image_id}.png').convert('RGB')) \n",
    "        sample['image_id'] = image_id\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rects(sample, save_path = None):\n",
    "    result = sample['image'].copy()\n",
    "    for box, label in zip(sample['boxes'], sample['labels']):\n",
    "        color = (0, 0, 255)\n",
    "        if label == 2:\n",
    "            color = (0, 255, 0)\n",
    "        elif label == 3:\n",
    "            color = (255, 0, 0)\n",
    "        cv2.rectangle(result, (box[0],box[1]), (box[2], box[3]), color, 2)\n",
    "    \n",
    "    if save_path:\n",
    "        im = Image.fromarray(result)\n",
    "        im.save(save_path)\n",
    "\n",
    "    plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    for sample in dataset:\n",
    "        sample['image'] = sample['image'] / 255.\n",
    "\n",
    "def standarize_dataset(dataset):\n",
    "    mean = np.mean([sample['image'].mean() for sample in dataset])\n",
    "    std_dev = np.std([sample['image'].std() for sample in dataset])\n",
    "    for sample in dataset:\n",
    "        sample['image'] = (sample['image'] - mean) / std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_augumentation(sample):\n",
    "    augumented = copy.deepcopy(sample)\n",
    "    type = random.randint(0,31)\n",
    "    \n",
    "    bbs = []\n",
    "    for bb in augumented['boxes']:\n",
    "        bbs.append(BoundingBox(x1=bb[0], x2=bb[2], y1=bb[1], y2=bb[3]))\n",
    "    bbs = BoundingBoxesOnImage(bbs, shape=augumented['image'].shape)\n",
    "    \n",
    "    params = {'fit_output': True}\n",
    "    if type & 0x1:\n",
    "        params['rotate'] = (-random.randint(1,15), random.randint(1,15))\n",
    "    if type & 0x2:\n",
    "        params['translate_percent'] ={\"x\": random.uniform(0,0.2), \"y\": random.uniform(0,0.2)}\n",
    "    if type & 0x4:\n",
    "        params['shear'] =(0,random.randint(1,10))\n",
    "    if type & 0x8:\n",
    "        flip_hr=iaa.Fliplr(p=1.0)\n",
    "        augumented['image'], bbs = flip_hr(image = augumented['image'], bounding_boxes=bbs)\n",
    "    if type & 0x10:\n",
    "        params['scale'] = random.uniform(1.1,1.3)\n",
    "    \n",
    "    aug = iaa.Affine(**params) \n",
    "    augumented['image'], bbs = aug(image = augumented['image'], bounding_boxes=bbs)\n",
    "    for i, bb in enumerate(bbs):\n",
    "        augumented['boxes'][i] = [bb.x1, bb.y1, bb.x2, bb.y2]\n",
    "    \n",
    "    #remove bbs outside the image\n",
    "    to_delete = list()\n",
    "    for i, bb in enumerate(augumented['boxes']):\n",
    "        if (not(0.< bb[0] <= augumented['image'].shape[1])) or \\\n",
    "        (not(0.< bb[2] <= augumented['image'].shape[1])) or \\\n",
    "        (not(0.< bb[1] <= augumented['image'].shape[0])) or \\\n",
    "        (not(0.< bb[3] <= augumented['image'].shape[0])):\n",
    "            to_delete.append(i)\n",
    "        elif bb[0] >= bb[2] or bb[1] >= bb[3]:\n",
    "            to_delete.append(i)\n",
    "            \n",
    "    for index in sorted(to_delete, reverse=True):\n",
    "        del augumented['boxes'][index]\n",
    "        del augumented['labels'][index]\n",
    "        \n",
    "    return augumented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_by_id(ids, path = '.'):\n",
    "    n = len(os.listdir(f'{path}/annotations/'))\n",
    "    dataset = []\n",
    "    for i in ids:\n",
    "        dataset.append(get_sample(path, i))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT1\n",
    "def get_split_1(train_ids, test_ids, val_ids):\n",
    "    return load_dataset_by_id(train_ids), load_dataset_by_id(test_ids), load_dataset_by_id(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT2\n",
    "def get_agumenteted_data(samples):\n",
    "    new_samples = []\n",
    "    for sample in samples:\n",
    "        new_samples.append(sample)\n",
    "        if (3 in sample[\"labels\"] or 1 in sample[\"labels\"]) and 2 in sample[\"labels\"]:\n",
    "            for _ in range(1):\n",
    "                new_samples.append(random_augumentation(sample)) \n",
    "        elif 3 in sample[\"labels\"]:\n",
    "            for _ in range(12):\n",
    "                new_samples.append(random_augumentation(sample)) \n",
    "        elif 1 in sample[\"labels\"]:\n",
    "            for _ in range(4):\n",
    "                new_samples.append(random_augumentation(sample)) \n",
    "    return new_samples\n",
    "    \n",
    "def get_split_2(train_ids, test_ids, val_ids):\n",
    "    train_samples = get_agumenteted_data(load_dataset_by_id(train_ids))\n",
    "    test_samples = get_agumenteted_data(load_dataset_by_id(test_ids))\n",
    "    val_samples = get_agumenteted_data(load_dataset_by_id(val_ids))\n",
    "\n",
    "    normalize_dataset(train_samples)\n",
    "    standarize_dataset(train_samples)\n",
    "    \n",
    "    normalize_dataset(test_samples)\n",
    "    standarize_dataset(test_samples)\n",
    "    \n",
    "    normalize_dataset(val_samples)\n",
    "    standarize_dataset(val_samples)\n",
    "    \n",
    "    return train_samples, test_samples, val_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT3\n",
    "def get_split_3(train_samples, test_samples, val_samples):\n",
    "    return train_samples + val_samples, test_samples, val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "annotations_id_train, annotations_id_test = train_test_split(range(853), test_size=0.2)\n",
    "annotations_id_train, annotations_id_val = train_test_split(annotations_id_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped(iterable, n):\n",
    "    \"s -> (s0,s1,s2,...sn-1), (sn,sn+1,sn+2,...s2n-1), (s2n,s2n+1,s2n+2,...s3n-1), ...\"\n",
    "    return zip(*[iter(iterable)]*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(img):\n",
    "    return torch.tensor(img.transpose((2, 0, 1)), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def train_epoch(model, data, optimizer, logger, accumulation_steps):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    i = 0    \n",
    "    total_loss = 0\n",
    "    accu_step_loss = 0.\n",
    "    for batch in grouped(data, BATCH_SIZE):\n",
    "        i += 1\n",
    "        imgs = list(image_to_tensor(s['image']).to(device) for s in batch)\n",
    "        annotations = [{k: torch.tensor(s[k]).to(device) for k in s.keys() - {'image'}} for s in batch]\n",
    "        \n",
    "        for a in annotations:\n",
    "            if a['boxes'].shape[-1] != 4:\n",
    "                 a['boxes'].resize_((1,4))\n",
    "                 a['labels'].type(dtype=torch.int64)\n",
    "        \n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        total_loss += losses.item()\n",
    "        accu_step_loss += losses.item()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            training_stats = accu_step_loss / accumulation_steps\n",
    "            accu_step_loss = 0.\n",
    "            logger.log_metrics({\"accu_step_loss\": training_stats})\n",
    "        \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    avg_loss = total_loss / (len(train)//BATCH_SIZE)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    #model.eval()  # Turn   on the evaluation mode\n",
    "    model.train() # loss calculation is a bit complicated, \n",
    "                  # for validation purposes loss can be obtained from training mode.\n",
    "                  # https://stackoverflow.com/questions/60339336/validation-loss-for-pytorch-faster-rcnn/65347721#65347721\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            img = image_to_tensor(sample['image']).to(device)\n",
    "            annotation = {k: torch.tensor(sample[k]).to(device) for k in sample.keys() - {'image'}}\n",
    "            loss_dict = model([img], [annotation])\n",
    "            total_loss += sum(loss for loss in loss_dict.values()).item()\n",
    "    \n",
    "    return total_loss / len(data)\n",
    "\n",
    "\n",
    "def save_model(model, name = 'model-'):\n",
    "    path = \"./models/\"\n",
    "    dateTimeObj = datetime.now()\n",
    "    timestamp = dateTimeObj.strftime(\"%d-%b-%Y_%H:%M\")\n",
    "    filename = path + name + timestamp + '.pt'\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neptune_logging import NeptuneLogger, DummyLogger\n",
    "\n",
    "train, test, val = get_split_1(annotations_id_train, annotations_id_test, annotations_id_val)\n",
    "\n",
    "num_epochs = 25\n",
    "BATCH_SIZE = 1 # no gpu space for more, we need to accumulate gradient, and update it every ACCUMULATE steps\n",
    "ACCUMULATE = 4\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model_instance_segmentation(3).to(device)\n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.000001,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "best_loss = 10000\n",
    "\n",
    "logger = NeptuneLogger(\"Face mask - split 1\", None)\n",
    "best_loss = 10e5\n",
    "for i in range(num_epochs):\n",
    "    trainig_stats = train_epoch(model, train, optimizer, logger, ACCUMULATE)\n",
    "    val_stats = evaluate(model, val)\n",
    "    logger.log_metrics({\"training_loss\": trainig_stats,\n",
    "                        \"val_loss\": val_stats})\n",
    "    print('Epoch:', i + 1, \"Train loss: \", trainig_stats, \"Validation loss:\", val_stats)\n",
    "    \n",
    "    if val_stats < best_loss:\n",
    "        best_loss = val_stats\n",
    "        save_model(model, 'model_1_split')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neptune_logging import NeptuneLogger, DummyLogger\n",
    "\n",
    "train, test, val = get_split_2(annotations_id_train, annotations_id_test, annotations_id_val)\n",
    "\n",
    "num_epochs = 25\n",
    "BATCH_SIZE = 1 # no gpu space for more, we need to accumulate gradient, and update it every ACCUMULATE steps\n",
    "ACCUMULATE = 4\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model_instance_segmentation(3).to(device)\n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "best_loss = 10000\n",
    "\n",
    "logger = NeptuneLogger(\"Face mask - split 1\", None)\n",
    "best_loss = 10e5\n",
    "for i in range(num_epochs):\n",
    "    trainig_stats = train_epoch(model, train, optimizer, logger, ACCUMULATE)\n",
    "    val_stats = evaluate(model, val)\n",
    "    logger.log_metrics({\"training_loss\": trainig_stats,\n",
    "                        \"val_loss\": val_stats})\n",
    "    print('Epoch:', i + 1, \"Train loss: \", trainig_stats, \"Validation loss:\", val_stats)\n",
    "    \n",
    "    if val_stats < best_loss:\n",
    "        best_loss = val_stats\n",
    "        save_model(model, 'model_2_split')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neptune_logging import NeptuneLogger, DummyLogger\n",
    "\n",
    "train, test, val = get_split_2(annotations_id_train, annotations_id_test, annotations_id_val)\n",
    "train, test, val = get_split_3(train, test, val)\n",
    "\n",
    "num_epochs = 25\n",
    "BATCH_SIZE = 1 # no gpu space for more, we need to accumulate gradient, and update it every ACCUMULATE steps\n",
    "ACCUMULATE = 4\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model_instance_segmentation(3).to(device)\n",
    "\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "best_loss = 10000\n",
    "\n",
    "logger = NeptuneLogger(\"Face mask - split 1\", None)\n",
    "best_loss = 10e5\n",
    "for i in range(num_epochs):\n",
    "    trainig_stats = train_epoch(model, train, optimizer, logger, ACCUMULATE)\n",
    "    val_stats = evaluate(model, val)\n",
    "    logger.log_metrics({\"training_loss\": trainig_stats,\n",
    "                        \"val_loss\": val_stats})\n",
    "    print('Epoch:', i + 1, \"Train loss: \", trainig_stats, \"Validation loss:\", val_stats)\n",
    "    \n",
    "    if val_stats < best_loss:\n",
    "        best_loss = val_stats\n",
    "        save_model(model, 'model_3_split')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(3).to(device)\n",
    "model.load_state_dict(torch.load(\"models/MODEL2.pt\"))\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (face-mask)",
   "language": "python",
   "name": "face-mask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
