{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def get_bbox(obj):\n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def get_label(obj):\n",
    "    if obj.find('name').text == \"with_mask\":\n",
    "        return 2\n",
    "    elif obj.find('name').text == \"mask_weared_incorrect\":\n",
    "        return 3\n",
    "    return 1\n",
    "\n",
    "def get_sample(path, image_id): \n",
    "    with open(f'{path}/annotations/maksssksksss{image_id}.xml') as f:\n",
    "        data = f.read()\n",
    "        soup = BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "\n",
    "        num_objs = len(objects)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(get_bbox(i))\n",
    "            labels.append(get_label(i))\n",
    "\n",
    "        # Annotation is in dictionary format\n",
    "        sample = {}\n",
    "        sample[\"boxes\"] = boxes\n",
    "        sample[\"labels\"] = labels\n",
    "        sample['image'] = np.array(Image.open(f'images/maksssksksss{image_id}.png').convert('RGB')) \n",
    "        sample['image_id'] = image_id\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rects(sample, save_path = None):\n",
    "    result = sample['image'].copy()\n",
    "    for box, label in zip(sample['boxes'], sample['labels']):\n",
    "        color = (0, 0, 255)\n",
    "        if label == 2:\n",
    "            color = (0, 255, 0)\n",
    "        elif label == 3:\n",
    "            color = (255, 0, 0)\n",
    "        cv2.rectangle(result, (box[0],box[1]), (box[2], box[3]), color, 2)\n",
    "    \n",
    "    if save_path:\n",
    "        im = Image.fromarray(result)\n",
    "        im.save(save_path)\n",
    "\n",
    "    plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset):\n",
    "    for sample in dataset:\n",
    "        sample['image'] = sample['image'] / 255.\n",
    "\n",
    "def standarize_dataset(dataset):\n",
    "    mean = np.mean([sample['image'].mean() for sample in dataset])\n",
    "    std_dev = np.std([sample['image'].std() for sample in dataset])\n",
    "    for sample in dataset:\n",
    "        sample['image'] = (sample['image'] - mean) / std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_augumentation(sample):\n",
    "    augumented = copy.deepcopy(sample)\n",
    "    type = random.randint(0,31)\n",
    "    \n",
    "    bbs = []\n",
    "    for bb in augumented['boxes']:\n",
    "        bbs.append(BoundingBox(x1=bb[0], x2=bb[2], y1=bb[1], y2=bb[3]))\n",
    "    bbs = BoundingBoxesOnImage(bbs, shape=augumented['image'].shape)\n",
    "    \n",
    "    params = {'fit_output': True}\n",
    "    if type & 0x1:\n",
    "        params['rotate'] = (-random.randint(1,15), random.randint(1,15))\n",
    "    if type & 0x2:\n",
    "        params['translate_percent'] ={\"x\": random.uniform(0,0.2), \"y\": random.uniform(0,0.2)}\n",
    "    if type & 0x4:\n",
    "        params['shear'] =(0,random.randint(1,10))\n",
    "    if type & 0x8:\n",
    "        flip_hr=iaa.Fliplr(p=1.0)\n",
    "        augumented['image'], bbs = flip_hr(image = augumented['image'], bounding_boxes=bbs)\n",
    "    if type & 0x10:\n",
    "        params['scale'] = random.uniform(1.1,1.3)\n",
    "    \n",
    "    aug = iaa.Affine(**params) \n",
    "    augumented['image'], bbs = aug(image = augumented['image'], bounding_boxes=bbs)\n",
    "    for i, bb in enumerate(bbs):\n",
    "        augumented['boxes'][i] = [bb.x1, bb.y1, bb.x2, bb.y2]\n",
    "    \n",
    "    #remove bbs outside the image\n",
    "    to_delete = list()\n",
    "    for i, bb in enumerate(augumented['boxes']):\n",
    "        if (not(0.< bb[0] <= augumented['image'].shape[1])) or \\\n",
    "        (not(0.< bb[2] <= augumented['image'].shape[1])) or \\\n",
    "        (not(0.< bb[1] <= augumented['image'].shape[0])) or \\\n",
    "        (not(0.< bb[3] <= augumented['image'].shape[0])):\n",
    "            to_delete.append(i)\n",
    "        elif bb[0] >= bb[2] or bb[1] >= bb[3]:\n",
    "            to_delete.append(i)\n",
    "            \n",
    "    for index in sorted(to_delete, reverse=True):\n",
    "        del augumented['boxes'][index]\n",
    "        del augumented['labels'][index]\n",
    "        \n",
    "    return augumented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_by_id(ids, path = '.'):\n",
    "    n = len(os.listdir(f'{path}/annotations/'))\n",
    "    dataset = []\n",
    "    for i in ids:\n",
    "        dataset.append(get_sample(path, i))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_1(train_ids, test_ids, val_ids):\n",
    "    return load_dataset_by_id(train_ids), load_dataset_by_id(test_ids), load_dataset_by_id(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT2\n",
    "def get_agumenteted_data(samples):\n",
    "    new_samples = []\n",
    "    for sample in samples:\n",
    "        new_samples.append(sample)\n",
    "        if (3 in sample[\"labels\"] or 1 in sample[\"labels\"]) and 2 in sample[\"labels\"]:\n",
    "            for _ in range(1):\n",
    "                new_samples.append(random_augumentation(sample)) \n",
    "        elif 3 in sample[\"labels\"]:\n",
    "            for _ in range(12):\n",
    "                new_samples.append(random_augumentation(sample)) \n",
    "        elif 1 in sample[\"labels\"]:\n",
    "            for _ in range(4):\n",
    "                new_samples.append(random_augumentation(sample)) \n",
    "    return new_samples\n",
    "    \n",
    "def get_split_2(train_ids, test_ids, val_ids):\n",
    "    train_samples = get_agumenteted_data(load_dataset_by_id(train_ids))\n",
    "    test_samples = get_agumenteted_data(load_dataset_by_id(test_ids))\n",
    "    val_samples = get_agumenteted_data(load_dataset_by_id(val_ids))\n",
    "\n",
    "    normalize_dataset(train_samples)\n",
    "    standarize_dataset(train_samples)\n",
    "    \n",
    "    normalize_dataset(test_samples)\n",
    "    standarize_dataset(test_samples)\n",
    "    \n",
    "    normalize_dataset(val_samples)\n",
    "    standarize_dataset(val_samples)\n",
    "    \n",
    "    return train_samples, test_samples, val_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT3\n",
    "def get_split_3(train_samples, test_samples, val_samples):\n",
    "    return train_samples + val_samples, test_samples, val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "annotations_id_train, annotations_id_test = train_test_split(range(853), test_size=0.2)\n",
    "annotations_id_train, annotations_id_val = train_test_split(annotations_id_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.7.1+cu101\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped(iterable, n):\n",
    "    \"s -> (s0,s1,s2,...sn-1), (sn,sn+1,sn+2,...s2n-1), (s2n,s2n+1,s2n+2,...s3n-1), ...\"\n",
    "    return zip(*[iter(iterable)]*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(img):\n",
    "    return torch.tensor(img.transpose((2, 0, 1)), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def train_epoch(model, data, optimizer, logger, accumulation_steps, only_class = False):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    i = 0    \n",
    "    total_loss = 0\n",
    "    accu_step_loss = 0.\n",
    "    class_step_loss = 0.\n",
    "    for batch in grouped(data, BATCH_SIZE):\n",
    "        i += 1\n",
    "        imgs = list(image_to_tensor(s['image']).to(device) for s in batch)\n",
    "        annotations = [{k: torch.tensor(s[k]).to(device) for k in s.keys() - {'image'}} for s in batch]\n",
    "        \n",
    "        for a in annotations:\n",
    "            if a['boxes'].shape[-1] != 4:\n",
    "                 a['boxes'].resize_((1,4))\n",
    "                 a['labels'].type(dtype=torch.int64)\n",
    "        \n",
    "        loss_dict = model(imgs, annotations)\n",
    "        class_loss = loss_dict['loss_classifier']\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        if only_class:\n",
    "            class_loss.backward()\n",
    "        else:\n",
    "            losses.backward()\n",
    "        \n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        accu_step_loss += losses.item()\n",
    "        class_step_loss += class_loss.item()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            training_stats = accu_step_loss / accumulation_steps       \n",
    "            class_stats = class_step_loss / accumulation_steps\n",
    "            \n",
    "            accu_step_loss = 0.\n",
    "            class_step_loss = 0.\n",
    "            \n",
    "            if only_class:\n",
    "                class_lr = optimizer.param_groups[0]['lr']\n",
    "                lr = 0.0\n",
    "            else:\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                class_lr = 0.\n",
    "            logger.log_metrics({\"accu_step_loss\": training_stats,\n",
    "                                \"accu_class_loss\": class_stats,\n",
    "                                \"class_lr\": class_lr,\n",
    "                                \"lr\": lr})\n",
    "        \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    avg_loss = total_loss / (len(train)//BATCH_SIZE)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    #model.eval()  # Turn   on the evaluation mode\n",
    "    model.train() # loss calculation is a bit complicated, \n",
    "                  # for validation purposes loss can be obtained from training mode.\n",
    "                  # https://stackoverflow.com/questions/60339336/validation-loss-for-pytorch-faster-rcnn/65347721#65347721\n",
    "    total_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            img = image_to_tensor(sample['image']).to(device)\n",
    "            annotation = {k: torch.tensor(sample[k]).to(device) for k in sample.keys() - {'image'}}\n",
    "            loss_dict = model([img], [annotation])\n",
    "            total_loss += sum(loss for loss in loss_dict.values()).item()\n",
    "    \n",
    "    return total_loss / len(data)\n",
    "\n",
    "\n",
    "def save_model(model, name = 'model-'):\n",
    "    path = \"./models/\"\n",
    "    dateTimeObj = datetime.now()\n",
    "    timestamp = dateTimeObj.strftime(\"%d-%b-%Y_%H:%M\")\n",
    "    filename = path + name + timestamp + '.pt'\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, val = get_split_2(annotations_id_train, annotations_id_test, annotations_id_val)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = get_model_instance_segmentation(3).to(device)\n",
    "model.load_state_dict(torch.load(\"models/MODEL2.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(object):\n",
    "    def __init__(self, root):\n",
    "        self.data = root\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = image_to_tensor(self.data[idx]['image'])\n",
    "        image_id = torch.tensor([idx])\n",
    "        boxes = self.data[idx]['boxes']\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        num_objs = len(self.data[idx]['boxes'])\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = torch.as_tensor(self.data[idx]['labels'], dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([self.data[idx]['image_id']])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TorchDataset(test)\n",
    "train_dataset = TorchDataset(train)\n",
    "val_dataset = TorchDataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "train_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "val_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_metrics(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = 'Test:'\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = [\"bbox\"]\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "        \n",
    "        res = {target[\"image_id\"].item(): output for output, target in zip(outputs, [targets])}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Test:  [  0/326]  eta: 0:01:30  model_time: 0.1763 (0.1763)  evaluator_time: 0.0013 (0.0013)  time: 0.2785  data: 0.0892  max mem: 2507\n",
      "Test:  [100/326]  eta: 0:00:34  model_time: 0.1532 (0.1467)  evaluator_time: 0.0006 (0.0022)  time: 0.1594  data: 0.0017  max mem: 2507\n",
      "Test:  [200/326]  eta: 0:00:19  model_time: 0.1530 (0.1477)  evaluator_time: 0.0009 (0.0016)  time: 0.1513  data: 0.0016  max mem: 2507\n",
      "Test:  [300/326]  eta: 0:00:03  model_time: 0.1414 (0.1473)  evaluator_time: 0.0009 (0.0015)  time: 0.1457  data: 0.0017  max mem: 2507\n",
      "Test:  [325/326]  eta: 0:00:00  model_time: 0.1486 (0.1473)  evaluator_time: 0.0009 (0.0015)  time: 0.1498  data: 0.0016  max mem: 2507\n",
      "Test: Total time: 0:00:49 (0.1517 s / it)\n",
      "Averaged stats: model_time: 0.1486 (0.1473)  evaluator_time: 0.0009 (0.0015)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.189\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.277\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.160\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.215\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.264\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.084\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.188\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.276\n"
     ]
    }
   ],
   "source": [
    "test_eval = evaluate_metrics(model, data_loader_test, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_metrics(model, data_loader_train, device)\n",
    "evaluate_metrics(model, data_loader_val, device)\n",
    "evaluate_mectrics(model, data_loader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.neptune_logging import NeptuneLogger, DummyLogger\n",
    "\n",
    "train, test, val = get_split_2(annotations_id_train, annotations_id_test, annotations_id_val)\n",
    "val_dataset = TorchDataset(val)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "val_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "num_epochs = 25\n",
    "freezed_epochs = 4\n",
    "BATCH_SIZE = 1 # no gpu space for more, we need to accumulate gradient, and update it every ACCUMULATE steps\n",
    "ACCUMULATE = 3\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "psutil is not installed. You will not be able to abort this experiment from the UI.\n",
      "psutil is not installed. Hardware metrics will not be collected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/ziomm23/face-mask/e/FAC-30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All bounding boxes should have positive height and width. Found invalid box [0.0, 0.0, 0.0, 0.0] for target at index 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9d289a7d7e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfreezed_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrainig_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACCUMULATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrainig_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACCUMULATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-da09e701dbcf>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data, optimizer, logger, accumulation_steps, only_class)\u001b[0m\n\u001b[1;32m     17\u001b[0m                  \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mclass_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_classifier'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/face-mask/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/face-mask/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mbb_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegenerate_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mdegen_bb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbb_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     raise ValueError(\"All bounding boxes should have positive height and width.\"\n\u001b[0m\u001b[1;32m     93\u001b[0m                                      \u001b[0;34m\" Found invalid box {} for target at index {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                                      .format(degen_bb, target_idx))\n",
      "\u001b[0;31mValueError\u001b[0m: All bounding boxes should have positive height and width. Found invalid box [0.0, 0.0, 0.0, 0.0] for target at index 0."
     ]
    }
   ],
   "source": [
    "\n",
    "model = get_model_instance_segmentation(3).to(device)\n",
    "# parameters\n",
    "predictor_params = [p for p in model.roi_heads.box_predictor.parameters() if p.requires_grad]\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "predictor_optimizer = torch.optim.SGD(predictor_params, lr=0.0001,\n",
    "                                      momentum=0.9)\n",
    "optimizer = torch.optim.SGD(params, lr=0.0002,\n",
    "                                      momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=10,\n",
    "                                                   gamma=0.2)\n",
    "\n",
    "best_loss = 10000\n",
    "best_precision = 0\n",
    "\n",
    "logger = NeptuneLogger(\"Face mask - freez detection\", None)\n",
    "best_loss = 10e5\n",
    "for i in range(num_epochs):\n",
    "    if i < freezed_epochs:\n",
    "        trainig_stats = train_epoch(model, train, predictor_optimizer, logger, ACCUMULATE, True)\n",
    "    else:\n",
    "        trainig_stats = train_epoch(model, train, optimizer, logger, ACCUMULATE)\n",
    "        lr_scheduler.step()\n",
    "    val_stats = evaluate(model, val)\n",
    "    \n",
    "    test_eval = evaluate_metrics(model, data_loader_val, device)\n",
    "    import io\n",
    "    from contextlib import redirect_stdout\n",
    "\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        test_eval.coco_eval['bbox'].summarize()\n",
    "    result = f.getvalue()\n",
    "\n",
    "    result = result.split()\n",
    "    \n",
    "    logger.log_metrics({\"training_loss\": trainig_stats,\n",
    "                        \"val_loss\": val_stats,\n",
    "                        \"average_precision\": float(result[12]),\n",
    "                        \"average_recall\": float(result[117])})\n",
    "    \n",
    "    print('Epoch:', i + 1, \"Train loss: \", trainig_stats, \"Validation loss:\", val_stats)\n",
    "    \n",
    "    if val_stats < best_loss:\n",
    "        best_loss = val_stats\n",
    "        save_model(model, 'model_experiment')\n",
    "        \n",
    "    elif best_precision > float(result[12]):\n",
    "        best_precision = float(result[12])\n",
    "        save_model(model, 'model_experiment')\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(float(result[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (face-mask)",
   "language": "python",
   "name": "face-mask"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
